{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Transition model\n",
    "Describes the outcome of each action in each state.\n",
    "Here, the outcome is stochastic, so we write P(s'|s,a) to denote the probability of reaching state s' if action a is done in state s.\n",
    "\n",
    "We will assume that transitions are **Markovian**, the probability of reaching s' from s depends only on s and not on the history of earlier states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Markov decision process\n",
    "* a sequential decision problem\n",
    "    * fully observable environment\n",
    "    * stochastic environment\n",
    "* Markovian transition model\n",
    "* additive rewards\n",
    "\n",
    "Consists of a set of states (with an initial state s<sub>0</sub>); <br />\n",
    "A set *ACTIONS(s)* of actions in each state; <br />\n",
    "a transition model P(s<sup>1</sup>|s, a); <br />\n",
    "a reward function R(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Policy\n",
    "What the agent should do for any state that the agent might reach. <br />\n",
    "A policy is a function specifying an action for each state. <br />\n",
    "Policy is denoted by $\\pi$. <br />\n",
    "The action recommended by the policy $\\pi$ for state s is denoted by $\\pi(s)$\n",
    "\n",
    "**The quality of a policy is measured by the _expected_ utility of the possible environment histories generated by that policy.**\n",
    "\n",
    "**An optimal policy is a policy that yields the highest expected utility.**\n",
    "\n",
    "An optimal policy is denoted by $\\pi^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities over time\n",
    "###### Finite horizon\n",
    "It means that there is a *fixed* time N after which nothing matters. <br />\n",
    "*The optimal action in a given state could change over time.*\n",
    "###### Infinite horizon\n",
    "It means that there is no fixed deadline. <br />\n",
    "The optimal action depends only on the current state, and the optimal policy is **stationary**.\n",
    "#### How to calculate the utility of state sequences?\n",
    "##### Assigning utilities to sequences\n",
    "###### Additive rewards\n",
    "The utility of a state sequence is\n",
    "$$U_h([s_0, s_1, s_2,...]) = R(s_0) + R(s_1) + R(s_2) + ...$$\n",
    "###### Discounted rewards\n",
    "The utility of a state sequence is\n",
    "$$U_h([s_0, s_1, s_2,...]) = R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + ...$$\n",
    "$$ \\text{where } 0 < \\gamma \\leq 1 $$\n",
    "\n",
    "<span style='color:red'>**With discounted rewards, the utility of an infinite sequence is finite.**</span> <br />\n",
    "* $\\gamma < 1$\n",
    "* reward are bounded by $\\pm R_{max}$\n",
    "\n",
    "$$U_h([s_0, s_1, s_2,...]) = \\Sigma_{t=0}^\\infty \\gamma^t R(s_t) \\leq \\Sigma_{t=0}^\\infty \\gamma^t R_{max} = \\frac{R_{max}}{(1 - \\gamma)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The expected utility obtained by executing $\\pi$ starting in s\n",
    "$$\n",
    "\\mathbf{U}^\\pi(s) = \\mathbf{E} [ \\sum_{t=0}^\\infty \\gamma^t R(S_t) ]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The true utility of a state is just $ U^{\\pi *}(s)$\n",
    "Notice that U(s) and R(s) are quite different quantities; <br />\n",
    "* R(s) is the \"short term\" reward for being in s;\n",
    "* U(s) is the \"long term\" total reward from s onward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The action that maximizes the expected utility of the subsequent state\n",
    "$$\n",
    "\\pi^*(s) = argmax_{a \\in A(s)} \\Sigma_{s^1} P(s^1 | s, a) U(s^1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating an optimal policy\n",
    "The utility of a state is given by\n",
    "$$\n",
    "U(s) = R(s) + \\gamma \\times \\max\\limits_{a \\in A(s)} \\Sigma_{s'} P(s' | s, a) U(s')\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "The transition model $P(s'|s, a)$ <br />\n",
    "* Denote the probability of reaching state $s'$ if action *a* is done in state *s*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman update\n",
    "$$\n",
    "U_{i+1}(s) \\leftarrow R(s) + \\gamma \\times \\max\\limits_{a \\in A(s)} \\Sigma_{s'} P(s'|s, a) U_i(s')\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WALL\n",
    "WALL = (1,1)\n",
    "# Reward\n",
    "R = -.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Terminals\n",
    "T01 = (0,3)\n",
    "T02 = (1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., -1.],\n",
       "       [ 0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init the environment\n",
    "values = np.zeros(12).reshape(3,4)\n",
    "values[0, 3] =  1.\n",
    "values[1, 3] = -1.\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.812,  0.868,  0.918,  1.   ],\n",
       "       [ 0.762,  0.   ,  0.66 , -1.   ],\n",
       "       [ 0.705,  0.655,  0.611,  0.388]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx, val in enumerate(values):\n",
    "    for idx2, v in enumerate(val):\n",
    "        current_state = (idx, idx2)\n",
    "        \n",
    "        # Possible actions\n",
    "        up    = (idx - 1 if idx - 1 >= 0 else idx, idx2)\n",
    "        up    = up if up != WALL else current_state\n",
    "        \n",
    "        down  = (idx + 1 if idx + 1 <= 2 else idx, idx2)\n",
    "        down  = down if down != WALL else current_state\n",
    "        \n",
    "        left  = (idx, idx2 -1 if idx2 -1 >= 0 else idx2)\n",
    "        left  = left if left != WALL else current_state\n",
    "        \n",
    "        right = (idx, idx2 +1 if idx2 +1 <= 3 else idx2)\n",
    "        right = right if right != WALL else current_state\n",
    "        \n",
    "        if current_state in [WALL, T01, T02]:\n",
    "            continue\n",
    "        \n",
    "        values[current_state] = R + max([[.8*values[up]    + .1*values[left] + .1*values[right]], # UP\n",
    "                                         [.8*values[down]  + .1*values[left] + .1*values[right]], # DOWN\n",
    "                                         [.8*values[left]  + .1*values[up]   + .1*values[down]],  # LEFT\n",
    "                                         [.8*values[right] + .1*values[up]   + .1*values[down]]])[0]\n",
    "values"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
