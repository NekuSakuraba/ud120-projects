{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz: Evaluate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In this and the following exercises, you'll be adding train test splits to the data\n",
    "# to see how it changes the performance of each classifier\n",
    "#\n",
    "# The code provided will load the Titanic dataset like you did in project 0, then train\n",
    "# a decision tree (the method you used in your project) and a Bayesian classifier (as\n",
    "# discussed in the introduction videos). You don't need to worry about how these work for\n",
    "# now. \n",
    "#\n",
    "# What you do need to do is import a train/test split, train the classifiers on the\n",
    "# training data, and store the resulting accuracy scores in the dictionary provided.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "X = pd.read_csv('titanic_data.csv')\n",
    "# Limit to numeric data\n",
    "X = X._get_numeric_data()\n",
    "# Separate the labels\n",
    "y = X['Survived']\n",
    "# Remove labels from the inputs, and age due to missing data\n",
    "del X['Age'], X['Survived']\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Data\n",
      "   PassengerId  Pclass  SibSp  Parch     Fare\n",
      "0            1       3      1      0   7.2500\n",
      "1            2       1      1      0  71.2833\n",
      "2            3       3      0      0   7.9250\n",
      "3            4       1      1      0  53.1000\n",
      "4            5       3      0      0   8.0500\n",
      "> Target\n",
      "0    0\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "Name: Survived, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print '> Data'\n",
    "print X[:5]\n",
    "print '> Target'\n",
    "print y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(668, 5) (668L,)\n",
      "Decision Tree has accuracy:  0.62331838565\n",
      "GaussianNB has accuracy:  0.641255605381\n"
     ]
    }
   ],
   "source": [
    "# TODO: split the data into training and testing sets,\n",
    "# using the standard settings for train_test_split.\n",
    "# Then, train and test the classifiers with your newly split data instead of X and y.\n",
    "\n",
    "x_train, x_test, y_train, y_test = cv.train_test_split(X, y)\n",
    "print x_train.shape, y_train.shape\n",
    "\n",
    "# The decision tree classifier\n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf1.fit(x_train, y_train)\n",
    "\n",
    "dt_score = accuracy_score(y_test, clf1.predict(x_test))\n",
    "print \"Decision Tree has accuracy: \", dt_score\n",
    "\n",
    "# The naive Bayes classifier\n",
    "clf2 = GaussianNB()\n",
    "clf2.fit(x_train, y_train)\n",
    "\n",
    "nb_score = accuracy_score(y_test, clf2.predict(x_test))\n",
    "print \"GaussianNB has accuracy: \", nb_score\n",
    "\n",
    "answer = { \n",
    " \"Naive Bayes Score\": dt_score, \n",
    " \"Decision Tree Score\": nb_score\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz: Build a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In this exercise, we'll use the Titanic dataset as before, train two classifiers and\n",
    "# look at their confusion matrices. Your job is to create a train/test split in the data\n",
    "# and report the results in the dictionary at the bottom.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "#from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv('titanic_data.csv')\n",
    "\n",
    "X = X._get_numeric_data()\n",
    "y = X['Survived']\n",
    "del X['Age'], X['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(668, 5) (668L,)\n",
      "Confusion matrix for this Decision Tree:\n",
      "[[91 39]\n",
      " [46 47]]\n",
      "GaussianNB confusion matrix:\n",
      "[[110  20]\n",
      " [ 53  40]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: split the data into training and testing sets,\n",
    "# using the default settings for train_test_split (or test_size = 0.25 if specified).\n",
    "# Then, train and test the classifiers with your newly split data instead of X and y.\n",
    "\n",
    "x_train, x_test, y_train, y_test = cv.train_test_split(X, y)\n",
    "print x_train.shape, y_train.shape\n",
    "\n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf1.fit(x_train,y_train)\n",
    "print \"Confusion matrix for this Decision Tree:\\n\",confusion_matrix(y_test,clf1.predict(x_test))\n",
    "\n",
    "clf2 = GaussianNB()\n",
    "clf2.fit(x_train,y_train)\n",
    "print \"GaussianNB confusion matrix:\\n\",confusion_matrix(y_test,clf2.predict(x_test))\n",
    "\n",
    "#TODO: store the confusion matrices on the test sets below\n",
    "\n",
    "confusions = {\n",
    " \"Naive Bayes\": confusion_matrix(y_test,clf2.predict(x_test)),\n",
    " \"Decision Tree\": confusion_matrix(y_test,clf1.predict(x_test))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Confusion Matrix\n",
    "Were the classifiers more likely to make mistakes that were false positives (expecting people to survive who did not survive) or false negatives (expecting people to die who did not die)?\n",
    "***\n",
    "False negatives were more common.\n",
    "> While the difference is fairly small for decision trees, naive Bayes seems to produce far more false negatives than false positives!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall vs Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As with the previous exercises, let's look at the performance of a couple of classifiers\n",
    "# on the familiar Titanic dataset. Add a train/test split, then store the results in the\n",
    "# dictionary provided.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "X = pd.read_csv('titanic_data.csv')\n",
    "\n",
    "X = X._get_numeric_data()\n",
    "y = X['Survived']\n",
    "del X['Age'], X['Survived']\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import recall_score as recall\n",
    "from sklearn.metrics import precision_score as precision\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(668, 5) (668L,)\n",
      "Decision Tree recall: 0.54 and precision: 0.59\n",
      "GaussianNB recall: 0.41 and precision: 0.73\n"
     ]
    }
   ],
   "source": [
    "# TODO: split the data into training and testing sets,\n",
    "# using the standard settings for train_test_split.\n",
    "# Then, train and test the classifiers with your newly split data instead of X and y.\n",
    "\n",
    "x_train, x_test, y_train, y_test = cv.train_test_split(X, y)\n",
    "print x_train.shape, y_train.shape\n",
    "\n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf1.fit(x_train, y_train)\n",
    "print \"Decision Tree recall: {:.2f} and precision: {:.2f}\".format(recall(y_test,clf1.predict(x_test)),\n",
    "                                                                  precision(y_test,clf1.predict(x_test)))\n",
    "\n",
    "clf2 = GaussianNB()\n",
    "clf2.fit(x_train, y_train)\n",
    "print \"GaussianNB recall: {:.2f} and precision: {:.2f}\".format(recall(y_test,clf2.predict(x_test)),\n",
    "                                                               precision(y_test,clf2.predict(x_test)))\n",
    "\n",
    "results = {\n",
    "  \"Naive Bayes Recall\": recall(y_test,clf2.predict(x_test)),\n",
    "  \"Naive Bayes Precision\": precision(y_test,clf2.predict(x_test)),\n",
    "  \"Decision Tree Recall\": recall(y_test,clf1.predict(x_test)),\n",
    "  \"Decision Tree Precision\": precision(y_test,clf1.predict(x_test))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Algorithms\n",
    "In which ways is the decision tree outperforming Gaussian Naive Bayes?\n",
    "***\n",
    "The **Decision Tree** has a better recall, but worse precision.\n",
    "> **Naive Bayes** seems to do pretty well on precision, but rather poorly on recall!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz: Compute F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As usual, use a train/test split to get a reliable F1 score from two classifiers, and\n",
    "# save it the scores in the provided dictionaries.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "X = pd.read_csv('titanic_data.csv')\n",
    "\n",
    "X = X._get_numeric_data()\n",
    "y = X['Survived']\n",
    "del X['Age'], X['Survived']\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(668, 5) (668L,)\n",
      "Decision Tree F1 score: 0.51\n",
      "GaussianNB F1 score: 0.45\n"
     ]
    }
   ],
   "source": [
    "# TODO: split the data into training and testing sets,\n",
    "# using the standard settings for train_test_split.\n",
    "# Then, train and test the classifiers with your newly split data instead of X and y.\n",
    "\n",
    "x_train, x_test, y_train, y_test = cv.train_test_split(X, y)\n",
    "print x_train.shape, y_train.shape\n",
    "\n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf1.fit(x_train, y_train)\n",
    "print \"Decision Tree F1 score: {:.2f}\".format(f1_score(y_test, clf1.predict(x_test)))\n",
    "\n",
    "clf2 = GaussianNB()\n",
    "clf2.fit(x_train, y_train)\n",
    "print \"GaussianNB F1 score: {:.2f}\".format(f1_score(y_test, clf2.predict(x_test)))\n",
    "\n",
    "F1_scores = {\n",
    " \"Naive Bayes\": f1_score(y_test, clf2.predict(x_test)),\n",
    " \"Decision Tree\": f1_score(y_test, clf1.predict(x_test))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz: Compute Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "from sklearn.datasets import load_linnerud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linnerud_data = load_linnerud()\n",
    "X = linnerud_data.data\n",
    "y = linnerud_data.target\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree mean absolute error: 14.27\n",
      "Linear regression mean absolute error: 10.19\n"
     ]
    }
   ],
   "source": [
    "# TODO: split the data into training and testing sets,\n",
    "# using the standard settings for train_test_split.\n",
    "# Then, train and test the classifiers with your newly split data instead of X and y.\n",
    "\n",
    "from sklearn import cross_validation as cv\n",
    "\n",
    "x_train, x_test, y_train, y_test = cv.train_test_split(X, y)\n",
    "\n",
    "reg1 = DecisionTreeRegressor()\n",
    "reg1.fit(x_train, y_train)\n",
    "print \"Decision Tree mean absolute error: {:.2f}\".format(mae(y_test,reg1.predict(x_test)))\n",
    "\n",
    "reg2 = LinearRegression()\n",
    "reg2.fit(x_train, y_train)\n",
    "print \"Linear regression mean absolute error: {:.2f}\".format(mae(y_test,reg2.predict(x_test)))\n",
    "\n",
    "results = {\n",
    " \"Linear Regression\": mae(y_test,reg2.predict(x_test)),\n",
    " \"Decision Tree\": mae(y_test,reg1.predict(x_test))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz: Compute Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "from sklearn.datasets import load_linnerud\n",
    "\n",
    "linnerud_data = load_linnerud()\n",
    "X = linnerud_data.data\n",
    "y = linnerud_data.target\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree mean absolute error: 81.27\n",
      "Linear regression mean absolute error: 611.53\n"
     ]
    }
   ],
   "source": [
    "# TODO: split the data into training and testing sets,\n",
    "# using the standard settings for train_test_split.\n",
    "# Then, train and test the classifiers with your newly split data instead of X and y.\n",
    "\n",
    "from sklearn import cross_validation as cv\n",
    "\n",
    "x_train, x_test, y_train, y_test = cv.train_test_split(X, y)\n",
    "\n",
    "reg1 = DecisionTreeRegressor()\n",
    "reg1.fit(x_train, y_train)\n",
    "print \"Decision Tree mean absolute error: {:.2f}\".format(mse(y_test, reg1.predict(x_test)))\n",
    "\n",
    "reg2 = LinearRegression()\n",
    "reg2.fit(x_train, y_train)\n",
    "print \"Linear regression mean absolute error: {:.2f}\".format(mse(y_test, reg2.predict(x_test)))\n",
    "\n",
    "results = {\n",
    " \"Linear Regression\": mse(y_test, reg2.predict(x_test)),\n",
    " \"Decision Tree\": mse(y_test, reg1.predict(x_test))\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
